{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2 · Tokenización con tiktoken\n",
    "Contar tokens de una noticia (title + description) y decidir si requiere chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt",
    "from src.rss_loader import load_rpp_rss",
    "from src.tokenization import build_article_text, count_tokens, should_chunk",
    "",
    "# 1) Cargar 50 items",
    "items = load_rpp_rss(limit=50)",
    "assert len(items) > 0, \"No se cargaron items del RSS\"",
    "",
    "# 2) Tomar el primero y construir el texto",
    "sample_text = build_article_text(items[0])",
    "sample_text[:300]  # vista previa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Contar tokens y decidir chunking",
    "n_tokens = count_tokens(sample_text, model_name=\"gpt-3.5-turbo\")",
    "need_chunk = should_chunk(n_tokens, context_limit=4096, safety_margin=512)",
    "print({\"tokens\": n_tokens, \"need_chunk\": need_chunk})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# 4) Conteo de tokens para las primeras 10 noticias
import pandas as pd

rows = []
for i, it in enumerate(items[:10]):
    txt = build_article_text(it)
    nt = count_tokens(txt, model_name="gpt-3.5-turbo")
    rows.append({
        "idx": i,
        "title": it["title"][:80],
        "tokens": nt,
        "need_chunk": should_chunk(nt, context_limit=4096, safety_margin=512)
    })

pd.DataFrame(rows)

